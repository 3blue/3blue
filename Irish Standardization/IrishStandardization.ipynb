{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"IrishStandardization.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"gaYcQqFvVZc7"},"source":["from nltk.translate.bleu_score import corpus_bleu\n","import re\n","import string\n","from random import uniform"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuyt0SJvY4cY"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vh4EAFSdYSAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612437683,"user_tz":300,"elapsed":23626,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"b1841186-6cd1-46b1-85f7-5071ac3358e9"},"source":["import os\n","from google.colab import drive \n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/NLP/Challenge_5')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"5DYIVrYfVZdB","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1630612441909,"user_tz":300,"elapsed":4229,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"26dc54c2-6de1-4f39-9029-0a6ccebd38d6"},"source":["source_raw = open(\"train-source.txt\", \"r\").read().split(\"\\n\")\n","target_raw = open(\"train-target.txt\", \"r\").read().split(\"\\n\")\n","source_raw.pop()   #to make last characater </s>\n","target_raw.pop()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"yGrE5539VZdF"},"source":["punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~`…”“'''\n","source = []\n","target = []\n","for w in source_raw:\n","    if not w in punc:\n","        source.append(w)\n","for w in target_raw:\n","    if not w in punc:\n","        target.append(w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jt62yy89VZdH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612442245,"user_tz":300,"elapsed":8,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"f2f53c7d-3b87-4037-d862-5427380f240f"},"source":["print(len(source))\n","print(len(target))\n","print(source[0])\n","print(target[0])\n","print(source[len(source)-1])\n","print(target[len(target)-1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["804680\n","804198\n","<s>\n","<s>\n","</s>\n","</s>\n"]}]},{"cell_type":"code","metadata":{"id":"E-iA9snnVZdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612442520,"user_tz":300,"elapsed":279,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"d36877b8-b7a4-4c5d-f661-edaae0f038a5"},"source":["source_vocab = set(source)\n","target_vocab = set(target)\n","source_vocab.remove(\"<s>\")\n","target_vocab.remove(\"<s>\")\n","source_vocab.remove(\"</s>\")\n","target_vocab.remove(\"</s>\")\n","print(len(source_vocab))\n","print(len(target_vocab))\n","print(len(source_vocab)*len(target_vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34582\n","29098\n","1006267036\n"]}]},{"cell_type":"code","metadata":{"id":"HKJVV9zYVZdN"},"source":["source_sent = []\n","target_sent = []\n","sentence = []\n","for token in source:\n","    if token == \"</s>\":\n","        source_sent.append(sentence)\n","        sentence = []\n","    elif token != \"<s>\":\n","        sentence.append(token)\n","for token in target:\n","    if token == \"</s>\":\n","        target_sent.append(sentence)\n","        sentence = []\n","    elif token != \"<s>\":\n","        sentence.append(token)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrTUEylNVZdQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612443301,"user_tz":300,"elapsed":8,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"6adde8d9-c426-4e2e-892a-8662ade5ddaf"},"source":["print(len(source_sent))\n","print(len(target_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["45171\n","45171\n"]}]},{"cell_type":"code","metadata":{"id":"s6q9e4eRVZdS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612443302,"user_tz":300,"elapsed":6,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"59f6f371-a0af-4d8d-f612-1fa3d3417fa1"},"source":["tot = 0\n","for i in range(len(source_sent)):\n","    if len(source_sent[i])==len(target_sent[i]):\n","        tot = tot + 1\n","        \n","print(tot/len(source_sent))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6842000398485754\n"]}]},{"cell_type":"code","metadata":{"id":"M9OUm2DKVZdV"},"source":["alignments = dict()\n","for s in source_vocab:\n","    alignments[s] = dict()   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOR1Iaq4VZdX"},"source":["source_chunks = []\n","target_chunks = []\n","for i in range(len(source_sent)):   #First pass for freebies and chunk generation. Separating by \"anchor\" words didn't work as well as I thought, so instead the chunks are just all words that don't have an identi\n","    s = source_sent[i]\n","    t = target_sent[i]\n","    if len(s) == len(t):\n","        for j in range(len(s)):\n","            alignments[s[j]][t[j]] = alignments[s[j]].get(t[j],0) + 1\n","    else:\n","        chunk_s = []\n","        chunk_t = []\n","        for w in s:\n","            if w in t:\n","                alignments[w][w] = alignments[w].get(w,0)\n","            else:\n","                chunk_s.append(w)\n","        for w in t:\n","            if not w in s:\n","                chunk_t.append(w)\n","        source_chunks.append(chunk_s)\n","        target_chunks.append(chunk_t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88sepeiiVZdZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612444219,"user_tz":300,"elapsed":7,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"df2f359f-eb18-4db4-d93f-c4069c8b5dc2"},"source":["print(len(source_chunks))\n","print(len(target_chunks))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["14265\n","14265\n"]}]},{"cell_type":"code","metadata":{"id":"czyd1jL9VZdb"},"source":["for i in range(len(source_chunks)):   #second pass to lazily process chunks\n","    for a in source_chunks[i]:\n","        best_word = \"\"\n","        best_count = -1\n","        for b in target_chunks[i]:\n","            if b in alignments[a]:\n","                if alignments[a][b] > best_count:\n","                    best_count = alignments[a][b]\n","                    best_word = b\n","        if best_count > -1:\n","            alignments[a][best_word] = alignments[a].get(best_word) + 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm39ye4Ranbp"},"source":["Now for the neural implementation."]},{"cell_type":"code","metadata":{"id":"zj2YLC_pY0ul"},"source":["batch_size = 64  # Batch size for training.\n","epochs = 1  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksx5JFhJasgm"},"source":["input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","def squish(a):\n","  sent = \"\"\n","  for i in range(len(a)):\n","    if a[i] != '<s>' and a[i] != '</s>':\n","      if i > 0 and i < len(a)-1:\n","        sent = sent + \" \"\n","      sent = sent + (a[i])\n","  return sent\n","\n","for x in source_sent:\n","  s = squish(x)\n","  for c in s:\n","    if c not in input_characters:\n","      input_characters.add(c)\n","  input_texts.append(s)\n","for y in target_sent:\n","  t = squish(y)\n","  for c in t:\n","    if not c in target_characters:\n","      target_characters.add(c)\n","  target_texts.append(\"\\t\" + t + \"\\n\")\n","  #target_texts.append(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IX9CDLujiHp6"},"source":["input_texts = input_texts[:num_samples]\n","target_texts = target_texts[:num_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9c-M30ThKLx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630612448710,"user_tz":300,"elapsed":2670,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"2318e647-9348-4ee2-8e2b-a6aa2451bb3b"},"source":["input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n","\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",")\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 10000\n","Number of unique input tokens: 90\n","Number of unique output tokens: 79\n","Max sequence length for inputs: 455\n","Max sequence length for outputs: 430\n"]}]},{"cell_type":"code","metadata":{"id":"Tq2556sPhcRo"},"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        if char == '\\t' or char == '\\n':\n","          continue\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htBwGUNGWIV1"},"source":["# Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RBFWyv1kWIV4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630613026085,"user_tz":300,"elapsed":574699,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"88e60178-7898-4e28-c4a7-0d73cc5eeb84"},"source":["model.compile(\n","    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.2,\n",")\n","# Save model\n","model.save(\"s2s\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 519s 4s/step - loss: 0.8317 - accuracy: 0.8374 - val_loss: 0.5454 - val_accuracy: 0.8512\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: s2s/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: s2s/assets\n"]}]},{"cell_type":"code","metadata":{"id":"hUpC9yPkWIV8"},"source":["# Define sampling models\n","# Restore the model and construct the encoder and decoder.\n","model = keras.models.load_model(\"s2s\")\n","\n","encoder_inputs = model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # input_2\n","decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n","decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_lstm = model.layers[3]\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","decoder_dense = model.layers[4]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    #target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Yg3b8B8VZde"},"source":["# Baseline leave sentence unchanged; BLEU score is still > 0.5!\n","def dumb_translation(source_sent):\n","  return source_sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgWtZFxxVZdh"},"source":["def from_scratch_translation(source_sent):   #greedy decoding\n","    translation = []\n","    for s in source_sent:\n","        if s in alignments and bool(alignments[s]):\n","            translation.append(max(alignments[s], key=alignments[s].get))\n","        else:\n","            translation.append(s)\n","    return translation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIro44yIVZdm"},"source":["# Slightly better baseline is to apply a few global spelling changes\n","def anything_goes_translation(source_sent):\n","  sent = squish(source_sent)\n","  s = \"\"\n","  for c in sent:\n","    if not c in punc:\n","      s = s + c\n","  v = np.zeros(\n","    (1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n","  )\n","  for t, char in enumerate(s):\n","    v[0, t, input_token_index[char]] = 1.0\n","  v[0, t + 1 :, input_token_index[\" \"]] = 1.0\n","  return decode_sequence(v[0:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9pA-oKPtlWk","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1630613053965,"user_tz":300,"elapsed":21626,"user":{"displayName":"Evan McKinnon","photoUrl":"","userId":"15729169813940064193"}},"outputId":"0d3af2f7-2333-410c-b110-14c918a32381"},"source":["anything_goes_translation([\"Cinnte\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Bh                                                                                                                                                                                                                                                                                                                                                                                                                                             '"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"tRWgEvZuVZdo"},"source":["def evaluate():\n","  testsource = open('test-source.txt', 'r')\n","  sentence = []\n","  dumb_hypotheses = []\n","  from_scratch_hypotheses = []\n","  anything_goes_hypotheses = []\n","  for line in testsource:\n","    token = line.rstrip(\"\\n\")\n","    if token == '<s>':\n","      sentence = []\n","    elif token == '</s>':\n","      dumb_hypotheses.append(dumb_translation(sentence))\n","      from_scratch_hypotheses.append(from_scratch_translation(sentence))\n","      anything_goes_hypotheses.append(anything_goes_translation(sentence))\n","    else:\n","      sentence.append(token)\n","  references = []\n","  testtarget = open('test-target.txt', 'r')\n","  for line in testtarget:\n","    token = line.rstrip(\"\\n\")\n","    if token == '<s>':\n","      sentence = []\n","    elif token == '</s>':\n","      references.append([sentence])\n","    else:\n","      sentence.append(token)\n","  return (corpus_bleu(references,dumb_hypotheses), corpus_bleu(references,from_scratch_hypotheses), corpus_bleu(references,anything_goes_hypotheses))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fphmRYUVZdq"},"source":["evaluate()"],"execution_count":null,"outputs":[]}]}